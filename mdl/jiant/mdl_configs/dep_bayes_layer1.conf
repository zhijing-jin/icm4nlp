

// This imports the defaults, which can be overridden below.
include "[path_to_the_repo]/jiant/config/defaults.conf"  // relative path to this file                  // CHANGE THIS!!!

data_dir = "[path_to_the_data]"  // Base directory in which to look for raw data subdirectories. This   // CHANGE THIS!!!
                                 // could be the glue_data directory created by download_glue_data.py.



project_dir = "."  // The base directory for model output.
exp_name = ""  // configure this
run_name = "run"  // default

edges-tmpl-bayes = ${edges-tmpl-large}{
    classifier_dropout = 0.0            // MDL: for Bayesian probe, we have to disable dropout (it just makes no sense)
    classifier_hid_dim = 256
}

// Task names: 
// "edges-pos-ontonotes", "edges-nonterminal-ontonotes", "edges-dep-ud-ewt", 
// "edges-srl-ontonotes", "edges-coref-ontonotes", "edges-rel-semeval", "edges-ner-ontonotes"

edges-dep-ud-ewt = ${edges-tmpl-bayes}    ------>    ------->   ------->   ----> // CHANGE THIS!!! (task name to the left)

pretrain_tasks = "edges-dep-ud-ewt"  // empty: don't run main training phase     // CHANGE THIS!!!
target_tasks = "edges-dep-ud-ewt"   // train classifier only                     // CHANGE THIS!!!

// Eval will use task-specific params.
do_pretrain = 1        // skip main train phase
allow_untrained_encoder_parameters = 1  // allow skipping training phase
allow_missing_task_map = 1  // ignore missing classifier_task_map.json
do_target_task_training = 0  // train using eval task params
write_preds = "val,test"

lr = 0.001  // Initial learning rate.
lr_patience = 5  // vals until LR decay
lr_decay_factor = 0.999999  // Factor by which to decay LR (multiplicative) when lr_patience is reached.
patience = 100      // vals until early-stopping

input_module = "elmo"
tokenizer = MosesTokenizer
elmo_layer_index = 1 // ------>    --------->    --------->    -------->           //  CHANGE THIS!!!
word_embs_file = "" // should not be necessary for BERT
max_seq_len = 512
cove = 0

// Use no-op encoder (no params).
sent_enc = "none"
skip_embs = 1  // forward embeddings from lower level.
sep_embs_for_skip = 1  // use task embeddings since we skip the generic ones.

//---------------------------------------------------------------------------
// MDL
do_full_eval = 1
classifier = bayes_mlp
clf_num_hid_layers = 1
classifier_dropout = 0.
train_type = SamplingMultiTaskTrainerBayes

edgeprobe_cnn_context = 0  // Expanded context for edge probing via CNN.
                           // 0 looks at only the current word, 1 adds +/-
                           // words (kernel width 3), etc.
                           
keep_all_checkpoints = 1  // If set, keep checkpoint files from every validation. Otherwise, keep
                          // only the best and (if different) most recent.
                          
min_epochs = 500












